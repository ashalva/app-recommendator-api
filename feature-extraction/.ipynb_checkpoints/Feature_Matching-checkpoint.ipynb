{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class MERGE_MODE(Enum):\n",
    "    DESCRIPTION=1\n",
    "    USER_REVIEWS=2\n",
    "    DESCRIPTION_USER_REVIEWS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Merge_Features:\n",
    "    def __init__(self,appId,mode,nlp):\n",
    "        self.appId =  appId\n",
    "        file_path = self.appId.upper() + \"_EXTRACTED_APP_FEATURES_\"\n",
    "        \n",
    "        if mode.value == MERGE_MODE.DESCRIPTION.value:\n",
    "            self.raw_app_features = self.GetExtractedFeatures(file_path + \"DESC.pkl\")\n",
    "        elif mode.value ==  MERGE_MODE.USER_REVIEWS.value:\n",
    "            self.raw_app_features = self.GetExtractedFeatures(file_path + \"REVIEWS.pkl\")\n",
    "        elif mode.value == MERGE_MODE.DESCRIPTION_USER_REVIEWS.value:\n",
    "            app_features_desc = self.GetExtractedFeatures(file_path + \"DESC.pkl\")\n",
    "            app_features_reviews = self.GetExtractedFeatures(file_path + \"REVIEWS.pkl\")\n",
    "            \n",
    "            self.raw_app_features = app_features_desc + app_features_reviews\n",
    "        \n",
    "        #print(self.raw_app_features)\n",
    "        self.nlp = nlp\n",
    "        \n",
    "    # matching on a single term level 'send email' and 'email send' are considered matching features\n",
    "    \n",
    "    def GetExtractedFeatures(self,path):\n",
    "        with open (path, 'rb') as fp:\n",
    "            raw_app_features = pickle.load(fp)\n",
    "        \n",
    "        return(raw_app_features)\n",
    "        \n",
    "    \n",
    "    def Merge_Matching_Terms(self):\n",
    "        \n",
    "        match_features=[]\n",
    "        \n",
    "        for feature1 in self.raw_app_features:\n",
    "            for feature2 in self.raw_app_features:\n",
    "                if feature1!=feature2 and (((feature1,feature2) in match_features)==False and ((feature2,feature1) in match_features)==False ) :\n",
    "                    feature1_words = feature1.split()\n",
    "                    feature2_words = feature2.split()\n",
    "                    if len(feature1_words) == len(feature2_words):\n",
    "                        contain_same_terms = all(w in feature1_words for w in feature2_words)\n",
    "                    \n",
    "                        if contain_same_terms==True:\n",
    "                            match_features.append((feature1,feature2))\n",
    "                            \n",
    "        #print(\"# of features left before term matching are %d\" % (len(self.raw_app_features)))\n",
    "        \n",
    "        # calcualte frequency and sort list by frequency\n",
    "        freq_dist = nltk.FreqDist(self.raw_app_features)\n",
    "        \n",
    "         #retain only the instance with highest frequency\n",
    "        for matched_feature in match_features:\n",
    "            feature1_freq = freq_dist[matched_feature[0]]\n",
    "            feature2_freq = freq_dist[matched_feature[1]]\n",
    "            \n",
    "            if feature1_freq>=feature2_freq:\n",
    "                del freq_dist[matched_feature[1]]\n",
    "            else:\n",
    "                del freq_dist[matched_feature[0]]\n",
    "        \n",
    "        # convert to list and sort app features by frequency\n",
    "        \n",
    "        app_features_freq = list(freq_dist.items())\n",
    "        self.app_features_freq = sorted(app_features_freq, key=lambda x: x[1],reverse=True)\n",
    "        \n",
    "        #print(\"# of features left after term matching are %d\" % (len(self.app_features_freq)))\n",
    "      \n",
    "        #print(\"\")\n",
    "        #print(\"Set of app features are ->\")\n",
    "        #print(self.app_features_freq)\n",
    "    \n",
    "    def FeatureExistinSameKey(self,key,new_feature,d):\n",
    "        found = False\n",
    "        #print(\"key->\",key)\n",
    "        #print(d.get(key,'na'))\n",
    "        #print(\"#######################\")\n",
    "        if d.get(key,'na') != 'na':\n",
    "            app_features_list = d[key]\n",
    "            if app_features_list is not None:\n",
    "                already_exist = any([app_feature[0]==new_feature for app_feature in app_features_list])\n",
    "                if already_exist==True:\n",
    "                    found = True\n",
    "        else:\n",
    "            found=True\n",
    "        \n",
    "        return(found)\n",
    "    \n",
    "    def FeatureExistinDictionary(self, new_feature,d):\n",
    "        found = False\n",
    "        for k,v in d.items():\n",
    "            app_features_list = d[k]\n",
    "            if app_features_list is not None:\n",
    "                already_exist = any([app_feature[0]==new_feature for app_feature in app_features_list])\n",
    "    \n",
    "                if already_exist==True:\n",
    "                    found = True\n",
    "                    break\n",
    "        \n",
    "        return(found)\n",
    "            \n",
    "    def Merge_Features_based_WordNet(self):\n",
    "        \n",
    "        feature_word_synonyms = self.GetFeatureSynonyms()\n",
    "        \n",
    "        self.feature_cluster={}\n",
    "        \n",
    "        for i in range(0,len(self.app_features_freq)):\n",
    "            app_feature_1 = self.app_features_freq[i]\n",
    "            feature1_words = app_feature_1[0].split()\n",
    "            \n",
    "            str_feature_1 = ' '.join(feature1_words)\n",
    "            \n",
    "            for j in range(0,len(self.app_features_freq)):\n",
    "                app_feature_2 = self.app_features_freq[j]\n",
    "                feature2_words = app_feature_2[0].split()\n",
    "                str_feature_2 = ' '.join(feature2_words)\n",
    "                found = 0\n",
    "                if i!=j and len(feature1_words)==len(feature2_words):\n",
    "                    for k in range(0,len(feature1_words)):\n",
    "                            syn_features_list=[]\n",
    "                            feature1_word = feature1_words[k]       \n",
    "                            feature2_word = feature2_words[k]\n",
    "                            \n",
    "                            syn_word_list = set(feature_word_synonyms[feature1_word])\n",
    "                                   \n",
    "                            if (feature2_word in syn_word_list)==True:\n",
    "                                found = found + 1\n",
    "                               \n",
    "                    #print(self.raw_app_features[j])\n",
    "                    #print(\"found = %d\" % (found))\n",
    "                \n",
    "                if len(feature1_words)==found and found ==len(feature1_words):\n",
    "                    if self.feature_cluster.get(str_feature_1,'na') == 'na' and self.FeatureExistinDictionary(str_feature_2,self.feature_cluster)==False:\n",
    "                        self.feature_cluster[str_feature_1]=[app_feature_1]\n",
    "                        feature_list = self.feature_cluster[str_feature_1]\n",
    "                        feature_list.append(app_feature_2)\n",
    "                        self.feature_cluster[str_feature_1]=feature_list\n",
    "                    elif self.feature_cluster.get(str_feature_1,'na') != 'na' and self.FeatureExistinDictionary(str_feature_2,self.feature_cluster)==False:\n",
    "                        feature_list = self.feature_cluster[str_feature_1]\n",
    "                        feature_list.append(app_feature_2)\n",
    "                        self.feature_cluster[str_feature_1]=feature_list\n",
    "            \n",
    "        # add remaining features\n",
    "            if self.feature_cluster.get(str_feature_1,'na')=='na' and self.FeatureExistinDictionary(str_feature_1,self.feature_cluster)==False:\n",
    "                self.feature_cluster[str_feature_1]=[app_feature_1]\n",
    "        \n",
    "        #print(\"group of synonyms app features are ->\")\n",
    "        #print(self.app_features_freq)\n",
    "        \n",
    "        #print(self.feature_cluster)\n",
    "    \n",
    "    def lemmalist(self,word):\n",
    "        syn_set = []\n",
    "        for synset in wn.synsets(word):\n",
    "            for item in synset.lemma_names():\n",
    "                syn_set.append(item)\n",
    "        return syn_set\n",
    "\n",
    "    def app_feature_words_vector(self,app_feature_words):\n",
    "        feature_words_vector = [word.vector for word in app_feature_words if word.has_vector]\n",
    "        if len(feature_words_vector)>0:\n",
    "            return np.mean(feature_words_vector, axis=0)\n",
    "        else:\n",
    "             return(np.array([]))\n",
    "    \n",
    "    def GetAppFeatureEmbeddings(self):\n",
    "        \n",
    "        app_features_embedding_dict={}\n",
    "        \n",
    "        for i in range(0,len(self.app_features_freq)):\n",
    "            app_feature = (self.app_features_freq[i])[0]\n",
    "            \n",
    "            app_feature_vector = self.app_feature_words_vector(self.nlp(app_feature))\n",
    "            \n",
    "            if app_feature_vector.size!=0:\n",
    "                app_features_embedding_dict[app_feature] = app_feature_vector\n",
    "            else:\n",
    "                app_features_embedding_dict[app_feature] = np.zeros(300)\n",
    "            \n",
    "        return(app_features_embedding_dict)\n",
    "        \n",
    "\n",
    "    def cluster_features_word_embeddings(self,similarity_threshold=.70):\n",
    "        app_features_embedding_dict = self.GetAppFeatureEmbeddings()\n",
    "        cosine = lambda v1, v2: np.dot(v1, v2) / (LA.norm(v1) * LA.norm(v2))\n",
    "        \n",
    "        self.feature_cluster_after_embedding=self.feature_cluster.copy()\n",
    "        \n",
    "        #print(\"\")\n",
    "        #print(\"group of app features are ->\")\n",
    "        \n",
    "        for group_key_1,grouped_app_features_1 in self.feature_cluster.items():\n",
    "            #print(group_key_1, \" -> \",grouped_app_features_1)\n",
    "            embedding_vector_group_key_1 = app_features_embedding_dict[group_key_1]\n",
    "            \n",
    "            for group_key_2,grouped_app_features_2 in self.feature_cluster.items():\n",
    "                if group_key_1!=group_key_2:\n",
    "                    embedding_vector_group_key_2 = app_features_embedding_dict[group_key_2]\n",
    "                    dist = cosine(embedding_vector_group_key_1, embedding_vector_group_key_2)\n",
    "                    \n",
    "                    if dist>similarity_threshold and self.FeatureExistinSameKey(group_key_1,group_key_2,self.feature_cluster_after_embedding)==False:\n",
    "                        #print('%s is simialr to %s' % (group_key_1,group_key_2))\n",
    "                        # merge similar groups \n",
    "                        if self.feature_cluster_after_embedding.get(group_key_1,'na')!='na' and self.feature_cluster_after_embedding.get(group_key_2,'na')!='na':\n",
    "                            app_features_group_1 = self.feature_cluster_after_embedding[group_key_1]                        \n",
    "                            app_features_group_2 = self.feature_cluster_after_embedding[group_key_2]\n",
    "                        \n",
    "                        \n",
    "                        # group 2 is simialr to group 1 so merge them\n",
    "                            app_features_group_1.extend(app_features_group_2)\n",
    "                            del self.feature_cluster_after_embedding[group_key_2]\n",
    "            \n",
    "            #print(\"#####################\")\n",
    "        \n",
    "    def PrintClusteredFeatures(self,app_features_cluster):\n",
    "        #print(\"###############Cluster of app features###########\")\n",
    "        for key_app_feature, cluster_app_features in  app_features_cluster.items():\n",
    "            print(key_app_feature, \" -> \", app_features_cluster[key_app_feature])\n",
    "            \n",
    "    def GetFeatureSynonyms(self):\n",
    "        feature_word_synonyms={}\n",
    "\n",
    "        for i in range(0,len(self.raw_app_features)):\n",
    "            #feature_tokens = self.nlp(self.raw_app_features[i])\n",
    "            feature_tokens = nltk.word_tokenize(self.raw_app_features[i])\n",
    "            \n",
    "            for feature_token in feature_tokens:\n",
    "                \n",
    "                syn_word_list= set()\n",
    "                syn_word_list.add(feature_token)\n",
    "                \n",
    "                if(feature_word_synonyms.get(feature_token,'na')=='na'):\n",
    "                    syn_word_list.update(set(self.lemmalist(feature_token)))\n",
    "                    feature_word_synonyms[feature_token] = syn_word_list\n",
    "\n",
    "        return(feature_word_synonyms)\n",
    "    \n",
    "    def GetAppFeatures_JSONformat(self):\n",
    "        filepath = self.appId.upper() + \"_EXTRACTED_FEATURES.txt\" \n",
    "        file = open(filepath, 'w')\n",
    "        \n",
    "        json_output={}\n",
    "        json_output['appID'] = self.appId\n",
    "        \n",
    "        list_app_features=[]\n",
    "        features_clusters = []\n",
    "        for key_app_feature, cluster_app_features in  self.feature_cluster_after_embedding.items():\n",
    "            feature_dict={}\n",
    "            feature_dict['cluster_name']=key_app_feature\n",
    "            cluster_features=[]\n",
    "            feature_cluster = []\n",
    "            for feature_info in cluster_app_features:\n",
    "                app_feature = feature_info[0]\n",
    "                app_feature_freq = feature_info[1]\n",
    "                cluster_features.append({'feature':app_feature,'frequency':app_feature_freq})\n",
    "                feature_cluster.append(app_feature)\n",
    "            \n",
    "            \n",
    "            feature_dict['cluster_features'] = cluster_features\n",
    "            #features_clusters.append()\n",
    "            \n",
    "            list_app_features.append(feature_dict)\n",
    "            file.write(\"%s\\n\" % (','.join(feature_cluster)))\n",
    "        \n",
    "        #print('list of app features')\n",
    "        #print(list_app_features)\n",
    "        \n",
    "        json_output['app_features'] = list_app_features\n",
    "                \n",
    "        \n",
    "        file.close()\n",
    "        \n",
    "        return(json_output)\n",
    "    \n",
    "    def Merge(self,similarity_threshold=.80):\n",
    "        self.Merge_Matching_Terms()\n",
    "        self.Merge_Features_based_WordNet()\n",
    "        #self.PrintClusteredFeatures(self.feature_cluster)\n",
    "        self.cluster_features_word_embeddings(similarity_threshold)\n",
    "        #self.PrintClusteredFeatures(self.feature_cluster_after_embedding)\n",
    "        return(self.GetAppFeatures_JSONformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set of app features are ->\n",
      "[('see profile pictures', 1), ('multiple account support', 1), ('send attachments', 1), ('search your mail', 1), ('multiple account', 1), ('auto-complete contact names', 1), ('badge screen options', 1), ('multiple accounts', 1), ('see profile', 1), ('real-time notifications', 1), ('mail with results', 1), ('contact names', 1), ('threaded conversations', 1), ('auto-complete contact', 1), ('pictures as part', 1), ('reporting spam', 1), ('spelling suggestions', 1), ('search that works', 1), ('deleting spam', 1), ('undo send', 1), ('support and search', 1), ('receive attachments', 1), ('read your mail', 1), ('lock screen options', 1)]\n",
      "{'auto-complete': {'auto-complete'}, 'results': {'results', 'effect', 'event', 'solvent', 'issue', 'solution', 'leave', 'outcome', 'resultant', 'termination', 'final_result', 'resolution', 'lead', 'result', 'upshot', 'consequence', 'ensue', 'resultant_role', 'answer'}, 'support': {'support', 'back_up', 'plunk_for', 'documentation', 'suffer', 'bread_and_butter', 'subscribe', 'musical_accompaniment', 'hold_up', 'substantiate', 'financial_support', 'endure', 'sustenance', 'tolerate', 'corroborate', 'funding', 'affirm', 'reinforcement', 'abide', 'fend_for', 'brook', 'patronize', 'patronise', 'backup', 'put_up', 'stick_out', 'defend', 'digest', 'reenforcement', 'stand', 'hold', 'livelihood', 'back', 'indorse', 'bear', 'backing', 'underpin', 'patronage', 'plump_for', 'bear_out', 'sustain', 'financial_backing', 'keep_going', 'confirm', 'accompaniment', 'supporting', 'endorse', 'keep', 'stomach', 'living'}, 'names': {'list', 'identify', 'distinguish', 'gens', 'refer', 'diagnose', 'epithet', 'figure', 'describe', 'discover', 'key', 'public_figure', 'advert', 'name', 'name_calling', 'cite', 'constitute', 'appoint', 'nominate', 'make', 'mention', 'key_out', 'bring_up', 'names', 'call'}, 'threaded': {'wind', 'weave', 'string', 'wander', 'thread', 'draw', 'meander', 'threaded'}, 'account': {'answer_for', 'write_up', 'business_relationship', 'account', 'describe', 'chronicle', 'account_statement', 'invoice', 'explanation', 'accounting', 'history', 'story', 'score', 'news_report', 'bill', 'report', 'calculate'}, 'as': {'A', 'vitamin_A', 'as', 'Eastern_Samoa', 'atomic_number_33', 'adenine', 'amp', 'deoxyadenosine_monophosphate', 'angstrom', 'As', 'antiophthalmic_factor', 'a', 'equally', 'angstrom_unit', 'American_Samoa', 'every_bit', 'axerophthol', 'AS', 'group_A', 'ampere', 'arsenic', 'type_A'}, 'attachments': {'adhesion', 'fastening', 'adherence', 'attachments', 'bond', 'affixation', 'fond_regard', 'attachment'}, 'reporting': {'account', 'reportage', 'describe', 'reporting', 'coverage', 'cover', 'report'}, 'spam': {'junk_e-mail', 'spam', 'Spam'}, 'works': {'whole_shebang', 'whole_kit', 'crop', 'influence', 'work_out', 'body_of_work', 'run', 'lick', 'study', 'make_for', 'employment', 'full_treatment', 'workplace', 'operate', 'wreak', 'figure_out', 'kit_and_caboodle', 'forge', 'knead', 'do_work', 'deeds', 'work', 'process', 'bring', 'workings', 'work_on', 'cultivate', 'plant', 'kit_and_boodle', 'shape', 'whole_works', 'play', 'ferment', 'act_upon', 'exploit', 'works', 'mold', 'sour', 'act', 'form', 'put_to_work', 'whole_kit_and_boodle', 'puzzle_out', 'mould', 'oeuvre', 'whole_kit_and_caboodle', 'industrial_plant', 'exercise', 'whole_caboodle', 'make', 'turn', 'piece_of_work', 'solve', 'function', 'go'}, 'send': {'get_off', 'institutionalize', 'send', 'transmit', 'charge', 'commit', 'transport', 'beam', 'ship', 'send_out', 'station', 'place', 'institutionalise', 'post', 'mail', 'broadcast', 'air', 'direct', 'send_off'}, 'search': {'seek', 'research', 'lookup', 'explore', 'look_for', 'look', 'hunt', 'search', 'hunting'}, 'undo': {'unmake', 'undo', 'loosen', 'unwrap', 'untie'}, 'mail': {'chain_armour', 'get_off', 'chain_mail', 'ring_mail', 'ring_armor', 'ring_armour', 'send', 'mail_service', 'post', 'chain_armor', 'mail', 'postal_service'}, 'multiple': {'multiple'}, 'and': {'and'}, 'see': {'picture', 'take_in', 'pick_up', 'figure', 'view', 'come_across', 'check', 'look', 'go_out', 'get_wind', 'discover', 'understand', 'visualize', 'consider', 'find', 'get_a_line', 'attend', 'escort', 'run_across', 'realise', 'hear', 'find_out', 'envision', 'regard', 'control', 'meet', 'ensure', 'go_through', 'witness', 'determine', 'examine', 'see_to_it', 'visualise', 'insure', 'date', 'encounter', 'catch', 'get_word', 'interpret', 'watch', 'take_care', 'image', 'see', 'go_steady', 'experience', 'learn', 'ascertain', 'visit', 'project', 'realize', 'assure', 'construe', 'run_into', 'reckon', 'fancy'}, 'pictures': {'picture', 'pictorial_matter', 'movie', 'motion_picture', 'figure', 'delineation', 'motion-picture_show', 'moving_picture', 'visualize', 'exposure', 'pictures', 'render', 'scene', 'ikon', 'envision', 'icon', 'characterisation', 'photograph', 'photo', 'visualise', 'show', 'depict', 'pic', 'video', 'word_picture', 'impression', 'image', 'see', 'word-painting', 'characterization', 'project', 'film', 'flick', 'moving-picture_show', 'depiction', 'mental_picture', 'picture_show', 'painting', 'fancy'}, 'notifications': {'presentment', 'notice', 'telling', 'notification', 'apprisal', 'notifications'}, 'spelling': {'write', 'spelling', 'import', 'spell', 'spell_out'}, 'profile': {'profile', 'visibility'}, 'lock': {'lock_in', 'mesh', 'interlace', 'ignition_lock', 'shut_up', 'shut_away', 'operate', 'ringlet', 'curl', 'lock_away', 'put_away', 'lock', 'lock_up', 'engage', 'lock_chamber', 'interlock', 'whorl'}, 'options': {'alternative', 'pick', 'options', 'choice', 'option', 'selection'}, 'real-time': {'real-time'}, 'read': {'take', 'learn', 'register', 'read', 'say', 'interpret', 'scan', 'understand', 'study', 'record', 'translate', 'show'}, 'badge': {'badge'}, 'contact': {'liaison', 'get_hold_of', 'contact_lens', 'adjoin', 'reach', 'middleman', 'touch', 'inter-group_communication', 'physical_contact', 'link', 'meet', 'get_through', 'tangency', 'impinging', 'striking', 'contact'}, 'deleting': {'deleting', 'edit', 'delete', 'blue-pencil', 'erase', 'cancel'}, 'conversations': {'conversation', 'conversations'}, 'with': {'with'}, 'accounts': {'answer_for', 'write_up', 'business_relationship', 'account', 'describe', 'chronicle', 'account_statement', 'invoice', 'explanation', 'accounting', 'history', 'story', 'score', 'news_report', 'bill', 'accounts', 'report', 'calculate'}, 'part': {'voice', 'partially', 'character', 'persona', 'component', 'role', 'section', 'depart', 'parting', 'disunite', 'separate', 'start_out', 'region', 'split', 'take_off', 'portion', 'start', 'component_part', 'piece', 'contribution', 'set_forth', 'constituent', 'set_off', 'division', 'break', 'set_out', 'office', 'break_up', 'theatrical_role', 'divide', 'percentage', 'partly', 'split_up', 'share', 'function', 'part'}, 'your': {'your'}, 'that': {'that'}, 'receive': {'welcome', 'invite', 'obtain', 'encounter', 'take_in', 'pick_up', 'meet', 'have', 'get', 'incur', 'find', 'receive', 'experience'}, 'suggestions': {'proffer', 'suggestion', 'hint', 'prompting', 'trace', 'hypnotism', 'mesmerism', 'proposition', 'suggestions'}, 'screen': {'blind', 'projection_screen', 'test', 'block_out', 'screen_door', 'screenland', 'covert', 'sort', 'shield', 'silver_screen', 'riddle', 'filmdom', 'cover', 'sieve', 'screen', 'screen_out', 'concealment', 'CRT_screen'}}\n",
      "see profile pictures  ->  [('see profile pictures', 1), ('see profile', 1)]\n",
      "send attachments  ->  [('send attachments', 1), ('receive attachments', 1)]\n",
      "search your mail  ->  [('search your mail', 1), ('mail with results', 1), ('read your mail', 1)]\n",
      "multiple account  ->  [('multiple account', 1), ('multiple account support', 1), ('multiple accounts', 1)]\n",
      "auto-complete contact names  ->  [('auto-complete contact names', 1), ('auto-complete contact', 1)]\n",
      "badge screen options  ->  [('badge screen options', 1), ('lock screen options', 1)]\n",
      "real-time notifications  ->  [('real-time notifications', 1)]\n",
      "contact names  ->  [('contact names', 1)]\n",
      "undo send  ->  [('undo send', 1)]\n",
      "pictures as part  ->  [('pictures as part', 1)]\n",
      "reporting spam  ->  [('reporting spam', 1)]\n",
      "spelling suggestions  ->  [('spelling suggestions', 1)]\n",
      "search that works  ->  [('search that works', 1), ('support and search', 1)]\n",
      "deleting spam  ->  [('deleting spam', 1)]\n",
      "threaded conversations  ->  [('threaded conversations', 1)]\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     app = \"422689480\"\n",
    "#     obj_merge_features = Merge_Features(app,MERGE_MODE.DESCRIPTION,nlp)\n",
    "#     obj_merge_features.Merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ut]",
   "language": "python",
   "name": "conda-env-ut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
