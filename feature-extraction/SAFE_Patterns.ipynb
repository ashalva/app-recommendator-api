{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "import nltk\n",
    "from Text_Preprocessing import TextProcessing\n",
    "import Text_Preprocessing\n",
    "import importlib\n",
    "import requests\n",
    "from langdetect import detect\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Text_Preprocessing' from '/home/faiz/Desktop/PhD_Work/main_components/app_features_extraction/SAFE_implementation/Text_Preprocessing.py'>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importlib.reload(Text_Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class EXTRACTION_MODE(Enum):\n",
    "    APP_DESCRIPTION = 1\n",
    "    USER_REVIEWS= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SAFE_Patterns:\n",
    "    def __init__(self,appId,clean_sents,extraction_mode):\n",
    "        self.appId=appId\n",
    "        #self.app_description = clean_sents\n",
    "        self.clean_sentences = clean_sents\n",
    "        self.extraction_mode = extraction_mode\n",
    "    \n",
    "    def ExtractFeatures_Analyzing_Sent_POSPatterns(self):\n",
    "        raw_features_sent_patterns,remaining_sents=self.Extract_AppFeatures_with_SentencePatterns()\n",
    "        raw_features_pos_patterns=self.Extract_AppFeatures_with_POSPatterns(remaining_sents)\n",
    "        extracted_app_features = raw_features_sent_patterns + raw_features_pos_patterns\n",
    "        \n",
    "        #print(extracted_app_features)\n",
    "        \n",
    "        #extracted_app_features=[feature for feature in extracted_app_features if 'including' not in feature.split()]\n",
    "        #extracted_app_features=[feature for feature in extracted_app_features if 'providing' not in feature.split()]\n",
    "        #extracted_app_features=[feature for feature in extracted_app_features if 'winning' not in feature.split()]\n",
    "        extracted_app_features=[feature for feature in extracted_app_features if 'iphone' not in feature.split()]\n",
    "        extracted_app_features=[feature for feature in extracted_app_features if 'more' not in feature.split()]\n",
    "        extracted_app_features=[feature for feature in extracted_app_features if 'many' not in feature.split()]\n",
    "        \n",
    "        #remvove features that has 1 char word in them\n",
    "        #clean_app_features= [feature for feature in extracted_app_features if detect(feature)=='en']\n",
    "           \n",
    "        list_clean_feaures=[]\n",
    "        # remove noise\n",
    "        for feature in extracted_app_features:\n",
    "            words = feature.split()\n",
    "            duplicate_words = all(x == words[0] for x in words)\n",
    "            \n",
    "            if duplicate_words!=True:\n",
    "                list_clean_feaures.append(feature)\n",
    "        \n",
    "        #print(\"# of extracted app features (after removing noise) are %d\" % (len(list_clean_feaures)))\n",
    "        \n",
    "        #print(list_clean_feaures)\n",
    "        \n",
    "        self.SaveExtractedFeatures(list_clean_feaures)\n",
    "        \n",
    "        #print(\"Extracted features saved sucessfully in a file!!!\")\n",
    "    \n",
    "    def SaveExtractedFeatures(self,extracted_features):\n",
    "        file_path = self.appId.upper() + \"_EXTRACTED_APP_FEATURES_\"\n",
    "        if self.extraction_mode.value == EXTRACTION_MODE.APP_DESCRIPTION.value:\n",
    "            file_path = file_path + \"DESC.pkl\"\n",
    "        elif self.extraction_mode.value == EXTRACTION_MODE.USER_REVIEWS.value:\n",
    "            file_path = file_path + \"REVIEWS.pkl\"\n",
    "        \n",
    "        with open(file_path, 'wb') as fp:\n",
    "            pickle.dump(extracted_features, fp)\n",
    "    \n",
    "    def Extract_Features_with_single_POSPattern(self,pattern_1,tag_text):\n",
    "        match_list = re.finditer(pattern_1,tag_text)\n",
    "        \n",
    "        app_features=[]\n",
    "            \n",
    "        for match in match_list:\n",
    "            app_feature = tag_text[match.start():match.end()]\n",
    "            feature_words= [w.split(\"/\")[0] for w in app_feature.split()]\n",
    "            app_features.append(' '.join(feature_words))\n",
    "        \n",
    "        return(app_features)\n",
    "    \n",
    "    def Extract_AppFeatures_with_POSPatterns(self,clean_sents):\n",
    "        app_features_pos_patterns=[]\n",
    "        \n",
    "        # list of all POS patterns\n",
    "        pos_patterns=[r\"[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\", # 1\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/(NOUN)\", # 2\n",
    "                     r\"[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/(NOUN)\", # 3\n",
    "                     r\"[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/CONJ\\s+[a-zA-Z-]+\\/(NOUN)\", # 4\n",
    "                     r\"[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN|VERB)\", # 5\n",
    "                     r\"[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN|ADJ|VERB)\\s+[a-zA-Z-]+\\/(NOUN)\", # 6\n",
    "                     r\"[a-zA-Z-]+\\/(VERB|NOUN)\\s+[a-zA-Z-]+\\/PRON\\s+[a-zA-Z-]+\\/(NOUN)\", # 7\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\", # 8\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/(NOUN)\", # 9\n",
    "                     r\"[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/(NOUN)\", # 10\n",
    "                     r\"[a-zA-Z-]+\\/(NOUN|ADJ)\\s+[a-zA-Z-]+\\/ADP\\s+[a-zA-Z-]+\\/(NOUN)\", # 11\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/(DET)\\s+[a-zA-Z-]+\\/(NOUN)\", # 12\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/ADP\\s+[a-zA-Z-]+\\/(NOUN)\", # 13\n",
    "                     r\"[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\", # 14\n",
    "                     r\"[a-zA-Z-]+\\/ADJ\\s+[a-zA-Z-]+\\/CONJ\\s+[a-zA-Z-]+\\/ADJ\", # 15\n",
    "                     r\"[a-zA-Z-]+\\/(VERB|NOUN)\\s+[a-zA-Z-]+\\/(PRON|DET)\\s+[a-zA-Z-]+\\/(ADJ|VERB|NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\", # 17\n",
    "                     r\"[a-zA-Z-]+\\/(VERB)\\s+[a-zA-Z-]+\\/(ADP)\\s+[a-zA-Z-]+\\/(ADJ|NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\", # rule # 16\n",
    "                     r\"[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/CONJ\\s+[a-zA-Z-]+\\/(NOUN)\\s+[a-zA-Z-]+\\/(NOUN)\" # rule # 18\n",
    "                     ]\n",
    "    #Noun Conjunction Noun Noun\n",
    "\n",
    "        for sent in clean_sents:\n",
    "            sent_tokens= nltk.word_tokenize(sent)\n",
    "            tag_tokens = nltk.pos_tag(sent_tokens,tagset='universal')\n",
    "            tag_text = ' '.join([word.lower() + \"/\" + tag for word,tag in tag_tokens])\n",
    "            #print(tag_text)\n",
    "            # extract app features through by iterating through list of all POS_patterns\n",
    "            for pattern in pos_patterns:\n",
    "                # store extracted features in list of app features\n",
    "               \n",
    "                raw_features = self.Extract_Features_with_single_POSPattern(pattern,tag_text)\n",
    "                if len(raw_features)!=0:\n",
    "                    #print(\"Pattern->%s\" % (pattern))\n",
    "                    #print(raw_features)\n",
    "                    app_features_pos_patterns.extend(raw_features)\n",
    "                \n",
    "                    #print(\"\")\n",
    "            \n",
    "        return(app_features_pos_patterns)\n",
    "    \n",
    "    def SentencePattern_Case1(self,tag_text):\n",
    "        raw_features=[]\n",
    "        #print(tag_text)\n",
    "        #print(\"\")\n",
    "        #tag_text = ' '.join([token.lower_  + \"/\" + token.pos_ for token in tokens])\n",
    "        #print(tag_text)\n",
    "        regex_case1 = r\"[a-zA-Z-]+\\/(VERB|NOUN)(\\s+,\\/.)?\\s+(and)\\/CONJ\\s+[a-zA-Z-]+\\/(VERB|ADJ|NOUN)(\\s+[a-zA-Z-]+\\/(NOUN|VERB))+\"\n",
    "        match = re.search(regex_case1,tag_text)\n",
    "        if match!=None:\n",
    "            matched_text= tag_text[match.start():match.end()]\n",
    "            words= [w.split(\"/\")[0] for w in matched_text.split() if w.split(\"/\")[1] not in ['.','CONJ']]\n",
    "            raw_features.append(words[0] + \" \" + ' '.join(words[2:]))\n",
    "            raw_features.append(words[1] + \" \" + ' '.join(words[2:]))\n",
    "\n",
    "        #print(raw_features)\n",
    "        #print(\"**************\")\n",
    "    \n",
    "        return(raw_features)\n",
    "    \n",
    "    def SentencePattern_Case2(self,tag_text):\n",
    "        raw_features=[]\n",
    "        regex_case2 = r\"[a-zA-Z-]+\\/(VERB|NOUN|ADJ)(\\s+[a-zA-Z-]+\\/PRON)?(\\s+[a-zA-Z-]+\\/(VERB|NOUN|ADJ)\\s+,\\/.)+(\\s+[a-zA-Z-]+\\/(VERB|NOUN|ADJ))?\\s+and\\/CONJ\\s+[a-zA-Z-]+\\/(VERB|NOUN|ADJ)\"\n",
    "        match=re.search(regex_case2,tag_text)\n",
    "        if match!=None:\n",
    "            matched_text= tag_text[match.start():match.end()]    \n",
    "            words = matched_text.split()\n",
    "\n",
    "            first_word = words[0].split(\"/\")[0]\n",
    "            last_word = words[len(words)-1].split(\"/\")[0]\n",
    "\n",
    "            enumeration_words = [w.split('/')[0] for index,w in enumerate(matched_text.split()) if index not in[0,len(words)-1] and w.split(\"/\")[1] not in ['.','CONJ','PRON']]\n",
    "            raw_features.append(first_word + \" \" + last_word)\n",
    "\n",
    "            raw_features += [first_word + \" \" + w2 for w2 in enumeration_words]\n",
    "\n",
    "        return(raw_features)\n",
    "    \n",
    "    def SentencePattern_Case3(self,tag_text):\n",
    "        raw_features=[]\n",
    "        #tag_text = ' '.join([token.lower_  + \"/\" + token.pos_ for token in tokens])\n",
    "        regex_case3 = r\"[a-zA-Z-]+\\/(VERB|NOUN)\\s+and\\/CONJ\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+and\\/CONJ\\s+[a-zA-Z-]+\\/(NOUN|VERB)\"\n",
    "        match = re.search(regex_case3,tag_text)\n",
    "        if match!=None:\n",
    "            matched_text= tag_text[match.start():match.end()]\n",
    "            words = matched_text.split()\n",
    "            words = [w.split(\"/\")[0] for w in words]\n",
    "            l1 = [words[0],words[2]]\n",
    "            l2 = [words[3],words[5]]\n",
    "            list_raw_features =list(itertools.product(l1,l2))\n",
    "            raw_features= [feature_words[0] + \" \" + feature_words[1] for feature_words in list_raw_features]\n",
    "            \n",
    "        return(raw_features)\n",
    "    \n",
    "    def SentencePattern_Case4(self,tag_text):\n",
    "        raw_features=[]\n",
    "        #tag_text = ' '.join([token.lower_  + \"/\" + token.pos_ for token in tokens])\n",
    "        regex_case4 = r\"[a-zA-Z-]+\\/(VERB|NOUN|ADJ)\\s+and\\/CONJ\\s+[a-zA-Z-]+\\/(VERB|NOUN|ADJ)\\s+[a-zA-Z-]+\\/ADP((\\s+[a-zA-Z-]+\\/(NOUN|VERB))(\\s+[a-zA-Z-]+\\/(ADP)))?\\s+[a-zA-Z-]+\\/(NOUN|VERB)\"\n",
    "        regex_case4 += \"(\\s+,\\/.)?\\s+(including\\/[a-zA-Z-]+)((\\s+[a-zA-Z-]+\\/(VERB|NOUN))+\\s+,\\/.)+\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+(and\\/CONJ)\\s+[a-zA-Z-]+\\/(NOUN|VERB)\"\n",
    "\n",
    "        match=re.search(regex_case4,tag_text)\n",
    "    \n",
    "        if match!=None:\n",
    "            matched_text= tag_text[match.start():match.end()]\n",
    "            words = matched_text.split()\n",
    "            words = [w.split(\"/\")[0] for w in words]\n",
    "\n",
    "            #words attach with first conjunction\n",
    "            feature_word1 = words[0]\n",
    "            feature_word2 = words[2]\n",
    "\n",
    "            feature_list1=[words[0],words[2]]\n",
    "\n",
    "            #feature words attach with second conjection\n",
    "            count=0\n",
    "            feature_list2=[]\n",
    "            fwords=[]\n",
    "            for i in range(3,len(words)):\n",
    "                if i<len(words)-1:\n",
    "                    if words[i+1]==\",\" and count==0:\n",
    "                        feature_list2.append(words[i])\n",
    "                        count = count + 1\n",
    "                    elif count==1:\n",
    "                        if words[i]!=\"including\" and words[i]!=',':\n",
    "                            fwords.append(words[i])\n",
    "                        if words[i] == \",\"  : \n",
    "                            if len(fwords)>0:\n",
    "                                feature_list2.append(' '.join(fwords))\n",
    "                            fwords=[]\n",
    "\n",
    "\n",
    "            feature_list2.append(words[len(words)-1])\n",
    "            feature_list2.append(words[len(words)-3])\n",
    "\n",
    "            list_raw_features = list(itertools.product(feature_list1,feature_list2))\n",
    "\n",
    "            raw_features= [feature_words[0] + \" \" + feature_words[1] for feature_words in list_raw_features]\n",
    "            \n",
    "        return(raw_features)\n",
    "    \n",
    "    def SentencePattern_Case5(self,tag_text):\n",
    "        raw_features=[]\n",
    "        #print(tag_text)\n",
    "        #tag_text = ' '.join([token.lower_  + \"/\" + token.pos_ for token in tokens])\n",
    "        regex_case5 = r\"[a-zA-Z-]+\\/(VERB|NOUN|ADP)\\s+,\\/.\\s+[a-zA-Z-]+\\/(VERB|NOUN)\\s+and\\/CONJ\\s+[a-zA-Z]+\\/(VERB|NOUN|ADJ)\\s+[a-zA-Z-]+\\/(NOUN|VERB|ADJ)\\s+(as\\/ADP)\\s+\"\n",
    "        regex_case5 += \"[a-zA-Z-]+\\/(ADJ|NOUN|VERB)(\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+,\\/.)+\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+(and\\/CONJ)\"\n",
    "        regex_case5 += \"\\s+[a-zA-Z-]+\\/(NOUN|VERB)\\s+[a-zA-Z-]+\\/(NOUN|VERB)\"\n",
    "        #regex_case5 += \"\\s+\\w+\\/(NOUN|VERB)\"  #\\s+\\w+\\/(NOUN|VERB)\"\n",
    "        match=re.search(regex_case5,tag_text)    \n",
    "        if match!=None:\n",
    "            match_text=tag_text[match.start():match.end()]\n",
    "            words_with_tags = match_text.split()\n",
    "            words = [w.split(\"/\")[0] for w in words_with_tags]\n",
    "\n",
    "            feature_list1=[words[0],words[2]]\n",
    "            feature_list2=[words[4] + \" \"  + words[5],words[7] + \" \" + words[8]]\n",
    "            feature_list3=[words[10],words[12],words[14] + \" \" + words[15]]\n",
    "            list_raw_features=list(itertools.product(feature_list1,feature_list3))\n",
    "            raw_features= [feature_words[0] + \" \" + feature_words[1] for feature_words in list_raw_features]\n",
    "            raw_features = raw_features + feature_list2\n",
    "        \n",
    "        return(raw_features)\n",
    "    \n",
    "    def Extract_AppFeatures_with_SentencePatterns(self):\n",
    "        raw_app_features_sent_patterns=[]\n",
    "        clean_sents_wo_sent_patterns=[]\n",
    "        \n",
    "        for sent in self.clean_sentences:\n",
    "            sent_tokens= nltk.word_tokenize(sent)\n",
    "            tag_tokens = nltk.pos_tag(sent_tokens,tagset='universal')\n",
    "            tag_text = ' '.join([word.lower()  + \"/\" + tag for word,tag in tag_tokens])\n",
    "            #print(tag_text)\n",
    "            sent_pattern_found=False\n",
    "            #case 1\n",
    "            raw_features_case1 = self.SentencePattern_Case1(tag_text)\n",
    "\n",
    "            if len(raw_features_case1)!=0:\n",
    "               \n",
    "                raw_app_features_sent_patterns.extend(raw_features_case1)\n",
    "                #print(raw_app_features_sent_patterns)\n",
    "                #print(\"CASE1***************************\")\n",
    "                #print(sent)\n",
    "                #print(\"CASE1***************************\")\n",
    "                sent_pattern_found=True\n",
    "            #case 2\n",
    "            raw_features_case2 = self.SentencePattern_Case2(tag_text)\n",
    "            \n",
    "            if len(raw_features_case2)!=0:\n",
    "                #print(\"CASE2***************************\")\n",
    "                #print(sent)\n",
    "                raw_app_features_sent_patterns.extend(raw_features_case2)\n",
    "                #print(\"CASE2***************************\")\n",
    "                sent_pattern_found=True\n",
    "            #case 3\n",
    "            raw_features_case3 = self.SentencePattern_Case3(tag_text)\n",
    "            if len(raw_features_case3)!=0:\n",
    "                raw_app_features_sent_patterns.extend(raw_features_case3)\n",
    "                sent_pattern_found=True\n",
    "                #print(\"CASE3***************************\")\n",
    "            #case 4\n",
    "            raw_features_case4 = self.SentencePattern_Case4(tag_text)\n",
    "            if len(raw_features_case4)!=0:                \n",
    "                raw_app_features_sent_patterns.extend(raw_features_case4)\n",
    "                print(raw_features_case4)\n",
    "                #print(\"***************************\")\n",
    "                sent_pattern_found=True\n",
    "            #case 5\n",
    "            raw_features_case5 = self.SentencePattern_Case5(tag_text)\n",
    "            if len(raw_features_case5)!=0:\n",
    "                raw_app_features_sent_patterns.extend(raw_features_case5)\n",
    "                #print(\"CASE5***************************\")\n",
    "                sent_pattern_found=True\n",
    "#                 print(raw_features_case5)\n",
    "#                 print(\"##########################\")\n",
    "            \n",
    "            if sent_pattern_found==False:\n",
    "                clean_sents_wo_sent_patterns.append(sent)\n",
    "        \n",
    "        \n",
    "        return(raw_app_features_sent_patterns,clean_sents_wo_sent_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enter alert', 'reminder alert', 'events reminders', 'dated reminders', 'list event', 'new event', 'set geofences', 'set dates', 'set times', 'locations repeating', 'better repeating', 'allows alerts', 'allows events', 'allows reminders', 'easiest add', 'fastest add', 'details dictation', 'use dictation', 'event show', 'reminder show', 'calendar app', 'week view', 'winning calendar', 'natural language', 'winning calendar app', 'new app', 'new parser', 'significant new parser', 'create reminders', 'starting your sentence', 'alerts phrases', 'create alerts', 'create alerts phrases', 'week view', 'extended keyboard', 'birthday options', 'birthday see', 'contact information', 'birthday see contact', 'tap on birthday', 'textexpander support', 'new events', 'adding new events', 'future looks', 'see your schedule', 'event list', 'easy find', 'makes easy find', 'event duplicate', 'hold an event', 'use search', 'specific events', 'locate specific events', 'facebook events', 'calendar services', 'including icloud', 'calendar app', 'google calendar', 'speak your event', 'including peek', '3d touch', 'accessibility support']\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     appName = \"DROP_BOX\"\n",
    "#     file_path = appName.lower() + \"_app_description.txt\"\n",
    "#     with open(file_path) as f:\n",
    "#         app_desc = f.readlines()\n",
    "    \n",
    "#     content = [x.strip() for x in app_desc] \n",
    "#     app_description = ' '.join(content)\n",
    "\n",
    "#     appID = \"718043190\"\n",
    "#     api_url='http://localhost:8081/app/description?id=' + appID\n",
    "#     myResponse = requests.get(api_url)\n",
    "#     if(myResponse.ok):\n",
    "#          app_data = json.loads((myResponse.content.decode('utf-8')))\n",
    "\n",
    "#     app_description = app_data['description'].strip()\n",
    "    \n",
    "#     obj_surf = SAFE_Patterns(appID,app_description,EXTRACTION_MODE.APP_DESCRIPTION)\n",
    "#     obj_surf.PreprocessData()\n",
    "#     obj_surf.ExtractFeatures_Analyzing_Sent_POSPatterns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ut]",
   "language": "python",
   "name": "conda-env-ut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
