{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "import spacy\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "from unidecode import unidecode\n",
    "import requests\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextProcessing:\n",
    "    def __init__(self,appName,data):\n",
    "        self.appName = appName\n",
    "        self.data = data\n",
    "\n",
    "    # segment description into sentences\n",
    "    def SegmemtintoSentences(self,sents_already_segmented=False):\n",
    "        self.sents=[]\n",
    "        #print(self.data)\n",
    "        if sents_already_segmented == True:     \n",
    "            list_lines = [line for line in self.data if line.strip()!='']\n",
    "            for line in list_lines:\n",
    "                u_line = unidecode(line)\n",
    "                pattern=r'\".*\"(\\s+-.*)?'\n",
    "                u_line = re.sub(pattern, '', u_line)\n",
    "                pattern1 = r'\\'s'\n",
    "                u_line= re.sub(pattern1,\"\",u_line)\n",
    "                #sentences = nltk.word_tokenize(u_line)\n",
    "                #print(u_line)\n",
    "                self.sents.append(u_line)\n",
    "        elif sents_already_segmented==False:\n",
    "            self.sents = nltk.sent_tokenize(self.data)\n",
    "            self.sents = [sent for sent in self.sents if sent.strip()!='']\n",
    "        \n",
    "        return(self.sents)\n",
    "        \n",
    "    # clean sentences\n",
    "    def  GetCleanSentences(self):\n",
    "        sentences=[]\n",
    "        clean_sentences=[]\n",
    "        sent_id=0\n",
    "        # remove explanations text with-in brackets\n",
    "        for sent in self.sents:\n",
    "            sent = html.unescape(sent.strip())\n",
    "            sent = sent.lstrip('-')\n",
    "            regex = r\"(\\(|\\[).*?(\\)|\\])\"\n",
    "            urls = re.findall('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?',sent)\n",
    "            #urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sent)\n",
    "            emails = re.findall(\"[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*\", sent) \n",
    "            url_regex = r'(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)?'\n",
    "            email_regex = r\"[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*\"\n",
    "            #quotations = re.findall('\"([^\"]*)\"', sent)\n",
    "            match_list = re.finditer(regex,sent)\n",
    "            match_list_email = re.finditer(email_regex,sent)\n",
    "            match_list_url = re.finditer(url_regex,sent)\n",
    "            new_sent=sent\n",
    "            \n",
    "            #print(match)\n",
    "            # filter sentences containg urls, emails, and quotations\n",
    "            if len(urls)==0 and len(emails)==0 :\n",
    "                if match_list:\n",
    "                    for match in match_list:\n",
    "                        txt_to_be_removed = sent[match.start():match.end()]\n",
    "                        new_sent=new_sent.replace(txt_to_be_removed,\"\")\n",
    "                    \n",
    "                    clean_sentences.append(new_sent)\n",
    "                else:\n",
    "                    clean_sentences.append(sent)\n",
    "            else:\n",
    "                if match_list_url:\n",
    "                    for match in match_list_url:\n",
    "                        txt_to_be_removed = sent[match.start()-1:match.end()]\n",
    "                        new_sent=new_sent.replace(txt_to_be_removed,\"\")\n",
    "                    \n",
    "                    clean_sentences.append(new_sent)\n",
    "                \n",
    "                elif match_list_email:\n",
    "                    for match in match_list_email:\n",
    "                        txt_to_be_removed = sent[match.start()-1:match.end()]\n",
    "                        new_sent=new_sent.replace(txt_to_be_removed,\"\")\n",
    "                    \n",
    "                    clean_sentences.append(new_sent)\n",
    "                \n",
    "        \n",
    "        \n",
    "        # replace bullet points and symbols # ,-, and * (use for delineate)        \n",
    "        pattern = r'\\*|\\u2022|#'\n",
    "        \n",
    "        app_names=[]\n",
    "        \n",
    "        app_words = self.appName.split(' ')\n",
    "        \n",
    "        if len(app_words)>1:\n",
    "            app_names.append(' '.join(app_words))\n",
    "            app_names.append(''.join(app_words))\n",
    "        else:\n",
    "            app_names.append(self.appName)\n",
    "        \n",
    "        #list_stop_words = list(stop_words.ENGLISH_STOP_WORDS) + list(stopwords.words('english')) + list(['anything','beautiful','efficient','enjoyable','way','quick','greeting','features','elegant','instant','fun']) \n",
    "                \n",
    "        #NON_STOP_WORDS=['see','out','thick','call','top','put','bill','under','get','next','all','cry','fill','found','interest','detail','empty','part','go','made','show','move','fire','up','find','keep'\\\n",
    "                                        #'toward','again','full','name','describe','back','amount','give','front','off','take','system','up','on','over','off','on','in','through','from','an','your','of','or','use','and']\n",
    "        \n",
    "        #final_stop_words = set(list_stop_words) - set(NON_STOP_WORDS)\n",
    "        custom_stop_words = ['anything','beautiful','efficient','enjoyable','way','quick','greeting','features','elegant','instant','fun','price','dropbox','iphone','total','is','in-app','apps','quickly','easily','lovely','others','other','own','the','interesting','addiction','following','featured','best','phone','sense','fantastical','fantastic','better',\n",
    "                            'award-winning','include','including','winning','improvements','improvement','significant','app','mac','pc','ipad','approach','application','applications','lets','several','safari','pro','google','matter','embarrassing','faster','mistakes','gmail','official','out','results','those','them','have','internet','anymore','are','provide','partial','useful','twitter','facebook','need','lose','it','yahoo','be','swiss','say','makes','make','local','button','will','vary','was','were','cloudapp','everything','straightforward','seamless','mundane','convenience','based','whatever','d','trials','trial','stuff','same','responsibility','love','great','would','good','only','might','strange','thing','nice','has','had','have','various','poor','stupid','could','did','does','do','doesn\\'t','didn\\'t','don\\'t','didnt','dont','doesnt','can','cant','couldn\\'t','couldnt','lot','alot','m','\\'ve','\\'ll','etc','am','lots','did','does','most','frightening','frighten','crash','crashes','bad','awesome','wonderful','simplistic','im','sometimes','should','shouldn\\'t','guys','me','enjoy','m','glitch','cute','having','em','i','this','bcoz','n','y','very','but','bt','cud','b','its','itz','it','good','goood','no','none','fine','plz','anyone','tell','problem','bug','issue','crash','please','fix','nd','awsme','juz','just','bugs','ur','dis','v','cnt','cool','c','bcz','fulfills','error','dat','canot','nthng','jst',\"\\'s\",\"anyways\",\"anywayz\",\"anyway\",\"thiz\",\"different\",\"things\",\"much\",\"wid\",\"about\",\"bore\",\"being\",\"excellent\",\"p\",\"plz\",\"whole\",\"allow\",\"allowed\",\"bein\",\"been\",\"confusion\",\"fixes\",\"wish\",\"hope\",\"needs\",\"brought\",\"ever\",\"worrying\",\"worry\",\"\\'s\",\"t\",\"s\" ,\"who\",\"whom\",\"whose\",\"which\",\"ll\",\"someone\",'certain','hate','company','everyone','first','few','issues','terrible','corrupted']\n",
    "        \n",
    "        \n",
    "        custom_stop_words = custom_stop_words + app_names\n",
    "        \n",
    "        for index,sent in enumerate(clean_sentences):\n",
    "            clean_sent= re.sub(pattern,\"\",sent)\n",
    "            # removing sub-ordinate clauses from a sentence\n",
    "            sent_wo_clause = self.Remove_SubOrdinateClause(clean_sent)\n",
    "        \n",
    "            clean_sentences[index] = sent_wo_clause\n",
    "            \n",
    "            tokens = nltk.word_tokenize(clean_sentences[index])\n",
    "                \n",
    "            #sent_tokens = [w.lower() for w in tokens if w.lower()]\n",
    "            sent_tokens = [w for w in tokens if w.lower() not in custom_stop_words]\n",
    "            #print(' '.join(sent_tokens))\n",
    "            #print(\"+++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "            sentences.append(' '.join(sent_tokens))\n",
    "        \n",
    "        \n",
    "                \n",
    "        return sentences\n",
    "    \n",
    "    def Remove_SubOrdinateClause(self,sentence):\n",
    "        sub_ordinate_words= ['when','after','although','because','before','if','rather','since',\\\n",
    "                            'though','unless','until','whenever','where','whereas','wherever','whether','while','why','which'\n",
    "                            ]\n",
    "        \n",
    "        sub_ordinate_clause = False\n",
    "        words=[]\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            if token.lower() in sub_ordinate_words: #and clause_has_obj==False and clause_has_sub==False:\n",
    "                sub_ordinate_clause = True\n",
    "    \n",
    "            if sub_ordinate_clause == False:\n",
    "                    words.append(token)\n",
    "                    #print(token.orth_,token.dep_)\n",
    "            elif sub_ordinate_clause == True:\n",
    "                break\n",
    "            \n",
    "        return(' '.join(words).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#     appName = \"test_app\"\n",
    "#     file_path = appName.lower() + \".txt\"\n",
    "#     with open(file_path) as f:\n",
    "#         app_desc = f.readlines()\n",
    "    \n",
    "#     #print(app_desc)\n",
    "#     content = [x.strip() for x in app_desc] \n",
    "#     app_description = ' '.join(content)\n",
    "\n",
    "#     appID = \"718043190\"\n",
    "#     api_url='http://localhost:8081/app/description?id=' + appID\n",
    "#     myResponse = requests.get(api_url)\n",
    "#     if(myResponse.ok):\n",
    "#          app_data = json.loads((myResponse.content.decode('utf-8')))\n",
    "\n",
    "#     app_description = app_data['description'].strip()\n",
    "\n",
    "    #app_description = unidecode(app_description)\n",
    "    \n",
    "#     textProcessor = TextProcessing(appID,app_description)\n",
    "#     textProcessor.SegmemtintoSentences(sents_already_segmented=True)\n",
    "#     clean_sentence=textProcessor.GetCleanSentences()\n",
    "    \n",
    "    #for sent in clean_sentence:\n",
    "        #print(sent)\n",
    "        #print(\"\")\n",
    "    #textProcessor.PrintCleanandPOSTagSentences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ut]",
   "language": "python",
   "name": "conda-env-ut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
